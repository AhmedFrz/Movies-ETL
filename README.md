Project Overview
In this project we are creating a data pipeline that moves data from a source to a destination, in this case, from jupyter notebook to pgAdmin. The ETL process creates data pipelines that also transform the data along the way. For this project we will work with data from Wikipedia and Kaggle files, transform the datasets by cleaning them up and joining them together, and load the cleaned dataset into a SQL database. This project is divided into four deliverables:

Deliverable 1
Create an ETL pipeline from raw data to a SQL database.

Deliverable 2
Extract data from disparate sources using Python.

Deliverable 3
Clean and transform data using Pandas. Use regular expressions to parse data and to transform text into numbers.

Deliverable 4
Load data with PostgreSQL.
